"""
Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu CSE-CIC-IDS2018:
- ƒê·ªçc file merged.csv ·ªü th∆∞ m·ª•c g·ªëc d·ª± √°n theo t·ª´ng chunk nh·ªè (tr√°nh h·∫øt RAM)
- N·∫øu d√≤ng n√†o l·ªói format s·∫Ω t·ª± ƒë·ªông b·ªè qua, kh√¥ng ti·ªÅn x·ª≠ l√Ω d√≤ng ƒë√≥
- Chu·∫©n h√≥a t√™n c·ªôt
- Lo·∫°i b·ªè c·ªôt kh√¥ng c·∫ßn
- Chu·∫©n h√≥a label (0: Benign, 1: Attack)
- Lo·∫°i b·ªè NaN, gi√° tr·ªã v√¥ c·ª±c, tr√πng l·∫∑p
- C√¢n b·∫±ng d·ªØ li·ªáu (n·∫øu c·∫ßn)
- √âp ki·ªÉu an to√†n tr∆∞·ªõc khi l∆∞u
- L∆∞u sang ƒë·ªãnh d·∫°ng Parquet ƒë·ªÉ train nhanh h∆°n
"""

import os
import numpy as np
import pandas as pd
from fastai.tabular.all import df_shrink

MERGED_CSV_PATH = "merged.csv"
PROCESSED_DATA_DIR = "data/processed"
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)

col_name_consistency = {
    # ...gi·ªØ nguy√™n nh∆∞ b·∫°n ƒë√£ mapping...
    # (b·∫°n c√≥ th·ªÉ copy l·∫°i ph·∫ßn mapping c·ªôt ·ªü tr√™n)
}

drop_columns = [
    "Flow ID", 'Fwd Header Length.1',
    "Source IP", "Src IP", "Source Port", "Src Port",
    "Destination IP", "Dst IP", "Destination Port", "Dst Port",
    "Timestamp"
]

def normalize_label(label):
    if str(label).strip().lower() in ["benign", "benign\n", "0", "normal"]:
        return 0
    return 1

def process_csv_in_chunks(path, chunk_size=200_000):
    chunk_paths = []
    chunk_iter = pd.read_csv(
        path, sep=",", encoding="utf-8", on_bad_lines='skip', low_memory=False, chunksize=chunk_size
    )
    for i, chunk in enumerate(chunk_iter):
        chunk.columns = chunk.columns.str.strip()
        chunk.rename(columns=col_name_consistency, inplace=True)
        chunk.drop(columns=drop_columns, inplace=True, errors="ignore")
        chunk['Label'] = chunk['Label'].replace({'BENIGN': 'Benign'})
        chunk['Label'] = chunk['Label'].apply(normalize_label)
        for col in chunk.columns:
            if col != 'Label':
                chunk[col] = pd.to_numeric(chunk[col], errors='coerce')
        if 'Protocol' in chunk.columns:
            chunk['Protocol'] = pd.to_numeric(chunk['Protocol'], errors='coerce').astype('Int32')
        chunk.replace([np.inf, -np.inf], np.nan, inplace=True)
        chunk.dropna(inplace=True)
        chunk.drop_duplicates(inplace=True)
        chunk.reset_index(drop=True, inplace=True)
        chunk = df_shrink(chunk)
        chunk_path = os.path.join(PROCESSED_DATA_DIR, f"chunk_{i}.parquet")
        chunk.to_parquet(chunk_path, index=False, engine="pyarrow")
        print(f"‚úÖ ƒê√£ l∆∞u chunk: {chunk_path}")
        chunk_paths.append(chunk_path)
    return chunk_paths

if __name__ == "__main__":
    if not os.path.exists(MERGED_CSV_PATH):
        print("‚ö† Kh√¥ng t√¨m th·∫•y file merged.csv ·ªü th∆∞ m·ª•c g·ªëc d·ª± √°n")
    else:
        chunk_paths = process_csv_in_chunks(MERGED_CSV_PATH, chunk_size=200_000)
        # G·ªôp l·∫°i th√†nh 1 file parquet l·ªõn v√† c√¢n b·∫±ng d·ªØ li·ªáu n·∫øu c·∫ßn
        dfs = [pd.read_parquet(p) for p in chunk_paths]
        df_all = pd.concat(dfs, ignore_index=True)
        min_count = min(df_all["Label"].value_counts().values)
        df_balanced = pd.concat([
            df_all[df_all["Label"] == 0].sample(min_count, random_state=42),
            df_all[df_all["Label"] == 1].sample(min_count, random_state=42)
        ], ignore_index=True)
        df_balanced = df_shrink(df_balanced)
        final_path = os.path.join(PROCESSED_DATA_DIR, "cic2018_processed.parquet")
        df_balanced.to_parquet(final_path, index=False)
        print(f"üéØ ƒê√£ l∆∞u file t·ªïng h·ª£p c√¢n b·∫±ng: {final_path}")